{"cells":[{"cell_type":"code","execution_count":1,"id":"d5ce1339","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5324,"status":"ok","timestamp":1667903079658,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"d5ce1339","outputId":"551eb046-909a-4df0-d8b1-e62c5a8b7e3e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"]},{"name":"stdout","output_type":"stream","text":["[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n"]}],"source":["# Importing libraries\n","import nltk\n","import time\n","import random\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n"," \n","# download the treebank corpus from nltk\n","nltk.download('treebank')\n"," \n","# download the universal tagset from nltk\n","nltk.download('universal_tagset')\n"," \n","# reading the Treebank tagged sentences\n","nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n"," \n","# print the first two sentences along with tags\n","print(nltk_data[:2])"]},{"cell_type":"code","execution_count":2,"id":"5139cc4d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1667903079658,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"5139cc4d","outputId":"cdbb93e1-69a5-4cf2-e4cf-f2e6d98b16ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["==============\n","('Pierre', 'NOUN')\n","('Vinken', 'NOUN')\n","(',', '.')\n","('61', 'NUM')\n","('years', 'NOUN')\n","('old', 'ADJ')\n","(',', '.')\n","('will', 'VERB')\n","('join', 'VERB')\n","('the', 'DET')\n","('board', 'NOUN')\n","('as', 'ADP')\n","('a', 'DET')\n","('nonexecutive', 'ADJ')\n","('director', 'NOUN')\n","('Nov.', 'NOUN')\n","('29', 'NUM')\n","('.', '.')\n","==============\n","('Mr.', 'NOUN')\n","('Vinken', 'NOUN')\n","('is', 'VERB')\n","('chairman', 'NOUN')\n","('of', 'ADP')\n","('Elsevier', 'NOUN')\n","('N.V.', 'NOUN')\n","(',', '.')\n","('the', 'DET')\n","('Dutch', 'NOUN')\n","('publishing', 'VERB')\n","('group', 'NOUN')\n","('.', '.')\n"]}],"source":["# print each word with its respective tag for first two sentences\n","for sent in nltk_data[:2]:\n","    print('==============')\n","    for tuple in sent:\n","        print(tuple,)"]},{"cell_type":"code","execution_count":3,"id":"2NhEJ0oWD303","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1667903079658,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"2NhEJ0oWD303","outputId":"8a92c7ca-0754-4caf-91ff-88abdffe3a3d"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nnew_nltk = [[] for i in range(len(nltk_data))]\\ni=0\\nfor sent in nltk_data:\\n  for tup in sent:\\n    if tup[1] == 'X' or tup[1] == '.':\\n      continue\\n    else:\\n      print(i)\\n      new_nltk[i].append(tup)\\n  i+=1\\nnltk_data = new_nltk\\n\""]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Experimental: Try removing the noise(X) and punctuations(.) tags to check for better accuracy\n","'''\n","new_nltk = [[] for i in range(len(nltk_data))]\n","i=0\n","for sent in nltk_data:\n","  for tup in sent:\n","    if tup[1] == 'X' or tup[1] == '.':\n","      continue\n","    else:\n","      print(i)\n","      new_nltk[i].append(tup)\n","  i+=1\n","nltk_data = new_nltk\n","'''"]},{"cell_type":"code","execution_count":4,"id":"da8b463b","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1667903079659,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"da8b463b"},"outputs":[],"source":["# split data into training and validation set in the ratio 80:20\n","train_set, test_set = train_test_split(nltk_data, train_size = 0.80, test_size = 0.20, random_state = 101)"]},{"cell_type":"code","execution_count":5,"id":"433a57bf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1667903079659,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"433a57bf","outputId":"fe7dcf06-470b-4a6e-d9cd-0464e5f2723f"},"outputs":[{"name":"stdout","output_type":"stream","text":["80310\n","20366\n"]}],"source":["# create list of train and test tagged words\n","train_tagged_words = [ tup for sent in train_set for tup in sent ]\n","test_tagged_words = [ tup for sent in test_set for tup in sent ]\n","\n","print(len(train_tagged_words))\n","print(len(test_tagged_words))"]},{"cell_type":"code","execution_count":6,"id":"47a8c1e9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1667903079659,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"47a8c1e9","outputId":"b0ed0fb4-5948-40db-cf8e-794b52c6dca6"},"outputs":[{"data":{"text/plain":["[('Drink', 'NOUN'),\n"," ('Carrier', 'NOUN'),\n"," ('Competes', 'VERB'),\n"," ('With', 'ADP'),\n"," ('Cartons', 'NOUN')]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# check some of the tagged words.\n","train_tagged_words[:5]"]},{"cell_type":"code","execution_count":7,"id":"d813ad7c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1667903079659,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"d813ad7c","outputId":"9dec2f4e-a0b1-42cc-e7f4-454a0c072ed6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of tags: 12\n","{'PRT', 'NUM', 'X', 'DET', 'CONJ', 'ADJ', 'VERB', '.', 'ADV', 'NOUN', 'PRON', 'ADP'}\n","Number of words in vocabulary: 11052\n"]}],"source":["# use set datatype to check how many unique tags are present in training data\n","tags = {tag for word, tag in train_tagged_words}\n","print('Number of tags:', len(tags))\n","print(tags)\n"," \n","# check total words in vocabulary\n","vocab = {word for word, tag in train_tagged_words}\n","print('Number of words in vocabulary:', len(vocab))"]},{"cell_type":"code","execution_count":8,"id":"0ba30571","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1667903079660,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"0ba30571"},"outputs":[],"source":["# compute Emission Probability\n","def word_given_tag(words, tags, train_bag = train_tagged_words):\n","    # your code here\n","\n","    # Create a dictionary of dictionaries to store the emission proabilities and dictionary of tags to store tag counts\n","    emission_matrix = {}\n","    tag_count = {}\n","    \n","    # Initialization\n","    for tag in tags:\n","      tag_count[tag] = 0.0\n","\n","    # Calculate emission count and tag count by iterating over train dataset\n","    for word,tag in train_bag:\n","      if word not in emission_matrix:\n","        emission_matrix[word] = {tg1:0.0 for tg1 in tags}  # Intitalize all tag values to zero for each new word found in dataset\n","      emission_matrix[word][tag]+=1\n","      tag_count[tag]+=1\n","\n","    # Calculate emission probabilities (emission_probability[word][tag] = (count of word,tag occurence in train_bag)/(count of tag occurence in train_bag))\n","    for word_tag_count_dict in emission_matrix.values():\n","      for tag in word_tag_count_dict.keys():\n","        word_tag_count_dict[tag] = word_tag_count_dict[tag]/tag_count[tag]\n","    \n","    # Initialize the prorabilities for unknown words in the test dataset to zero\n","    for word in words:\n","      if word not in emission_matrix:\n","        emission_matrix[word] = {tg1:1.0 for tg1 in tags}\n","    \n","    return emission_matrix\n","\n","\n","# compute Transition Probability\n","def t2_given_t1(tags, train_bag = train_tagged_words):\n","    # your code here\n","\n","    # Create a dictionary of dictionaries to store the transition probabilities and dictionary of tags to store tag counts\n","    transition_matrix = {}\n","    tag_count = {}\n","\n","    # Create a list of train dataset from 2nd word to the end to calculate transitions\n","    train_bag_next = train_bag[1:]\n","    \n","    # Inititalize tag counts and transition proability dictionary(matrix)\n","    for tag in tags:\n","      tag_count[tag] = 0.0\n","    \n","    for tag1 in tags:\n","      transition_matrix[tag1] = {}\n","      for tag2 in tags:\n","        transition_matrix[tag1][tag2] = 0.0\n","\n","    # Calculate transition counts and tag counts by iterating over train dataset\n","    for tup1,tup2 in list(zip(train_bag[:-1],train_bag_next)):\n","      tag_count[tup1[1]]+=1\n","      transition_matrix[tup1[1]][tup2[1]]+=1\n","    \n","    # Calculate transition probabilities(transition_probability[tag1][tag2] = (count of tag2 after tag1 occurence in train_bag)/(total count of all tags after tag1 occurence in train_bag))\n","    for tag1,next_tag_count in transition_matrix.items():\n","      for tag2 in next_tag_count.keys():\n","        next_tag_count[tag2] = next_tag_count[tag2]/tag_count[tag1]\n","\n","    return transition_matrix\n","\n","\n","\n","# create t x t transition matrix of tags, t = no of tags\n","# Matrix(i, j) represents P(jth tag after the ith tag)\n"," \n","#word_given_tag(vocab, tags, train_tagged_words).shape\n","#t2_given_t1(tags, train_tagged_words)"]},{"cell_type":"code","execution_count":9,"id":"ae1f51e2","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1667903079660,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"ae1f51e2"},"outputs":[],"source":["# you can also convert the matrix to a pandas dataframe for better readability"]},{"cell_type":"code","execution_count":10,"id":"a321eeee","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1667903079660,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"a321eeee"},"outputs":[],"source":["def Viterbi(words, train_bag = train_tagged_words):\n","    # your code here\n","\n","    # Create a list of tags\n","    tags = list({tag for word, tag in train_bag})\n","\n","    # Calculate emission and transition probabilities\n","    emission_matrix = word_given_tag(words, tags, train_bag)\n","    transition_matrix = t2_given_t1(tags, train_bag)\n","    \n","    # Intitalize the probability matrix and the bakctrack matrix\n","    tag_given_word_max_probability_matrix = np.array([[0.0 for i in range(len(words))] for j in range(len(tags))]) \n","    backtrack_best_path_matrix = np.array([['00000' for i in range(len(words))] for j in range(len(tags))])  # Take a string of 5 zeros as numpy stores unicode string of length <= initialized value\n","\n","    #  Calculate the initial probabilities for tags(hidden states) occuring in the train dataset\n","    tag_prob = {}\n","    for word,tag in train_bag:\n","      if tag in tag_prob:\n","        tag_prob[tag]+=1/len(train_bag)\n","      else:\n","        tag_prob[tag]=1/len(train_bag)\n","\n","    # Populate the probability matrix with prior probabilites in the 1st column (π vector)\n","    for i in range(len(tags)):\n","      tag_given_word_max_probability_matrix[i,0] = emission_matrix[words[0]][tags[i]]*tag_prob[tags[i]]\n","    \n","    # Normalize the probabilities (Since the values will keep on decreasing in each time step (i.e., with each word in sequence))\n","    tag_given_word_max_probability_matrix[:,0] = np.array([prob for prob in tag_given_word_max_probability_matrix[:,0]])\n","    tag_given_word_max_probability_matrix[:,0] = np.array([exp/np.sum(tag_given_word_max_probability_matrix[:,0]) for exp in tag_given_word_max_probability_matrix[:,0]])\n","\n","    # Calculate forward pass probabilities - O(N^2 * T)\n","    for i in range(1, tag_given_word_max_probability_matrix.shape[1]):\n","      for j in range(len(tags)):\n","        probability_of_tagj_given_prev_tag = [0.0]*len(tags)    # Initialize a list which will store the probability of transition to tag 'j' in layer 'i' from each element in layer 'i-1'\n","        for k in range(len(tags)):\n","          probability_of_tagj_given_prev_tag[k] = tag_given_word_max_probability_matrix[k,i-1]*transition_matrix[tags[k]][tags[j]]*emission_matrix[words[i]][tags[j]]\n","\n","        # Choose the max transition probability and argmax for tag 'j' from each tag in the previous layer\n","        tag_given_word_max_probability_matrix[j,i] = max(probability_of_tagj_given_prev_tag)\n","        backtrack_best_path_matrix[j,i] = tags[max(range(len(tags)), key=lambda x:probability_of_tagj_given_prev_tag[x])]\n","      \n","      # Normalize the probabilities (Since the values will keep on decreasing in each time step (i.e., with each word in sequence))\n","      tag_given_word_max_probability_matrix[:,i] = np.array([prob for prob in tag_given_word_max_probability_matrix[:,i]])\n","      tag_given_word_max_probability_matrix[:,i] = np.array([exp/np.sum(tag_given_word_max_probability_matrix[:,i]) for exp in tag_given_word_max_probability_matrix[:,i]])\n","    \n","    # Get the probability values from the backtrack matrix\n","    tags1 = {tags[i]:i for i in range(len(tags))}     # Create a mapping from tag to numeric index\n","    x = tags[np.argmax(tag_given_word_max_probability_matrix[:,-1])]  # Choose the 1st tag from argmax of probabilities of last layer\n","    res = [(words[backtrack_best_path_matrix.shape[1]-1], x)]\n","\n","    for i in range(backtrack_best_path_matrix.shape[1]-1,0,-1):       # Backtrack from the last element to 1st and pick the most likely sequence\n","      x = backtrack_best_path_matrix[:,i][tags1[x]]\n","      res.insert(0, (words[i-1], x))\n","\n","    return res\n","\n","#Viterbi(test_tagged_words)"]},{"cell_type":"code","execution_count":11,"id":"yN0TgGEkFBrA","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1667903079660,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"yN0TgGEkFBrA","outputId":"0c2b6a41-d96b-4e4b-a80c-b02b12d0f4ad"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\ntgs = ['X', '.', 'ADP', 'PRT', 'NUM', 'ADV', 'ADJ', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB']\\ntgs1 = {tgs[i]:i for i in range(len(tgs))}\\nx = tgs[np.argmax(pr[:,-1])]\\nres = [(test_run_base[btr.shape[1]-1][0], x)]\\n\\nfor i in range(btr.shape[1]-1,0,-1):\\n  x = btr[:,i][tgs1[x]]\\n  res.insert(0, (test_run_base[i-1][0], x))\\n\\nres\\n\""]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","tgs = ['X', '.', 'ADP', 'PRT', 'NUM', 'ADV', 'ADJ', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB']\n","tgs1 = {tgs[i]:i for i in range(len(tgs))}\n","x = tgs[np.argmax(pr[:,-1])]\n","res = [(test_run_base[btr.shape[1]-1][0], x)]\n","\n","for i in range(btr.shape[1]-1,0,-1):\n","  x = btr[:,i][tgs1[x]]\n","  res.insert(0, (test_run_base[i-1][0], x))\n","\n","res\n","'''"]},{"cell_type":"code","execution_count":12,"id":"5DIFj_e9GKJ_","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":711,"status":"ok","timestamp":1667903080365,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"5DIFj_e9GKJ_","outputId":"6712c70a-8699-4cab-f313-f05f7a040a80"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken in seconds: 0.27846264839172363\n","Viterbi Algorithm Accuracy: 95.21531100478468\n"]}],"source":["# test the Viterbi algorithm on a few sample sentences of test dataset\n","random.seed(1234)      # define a random seed to get same sentences when run multiple times\n"," \n","# choose random 10 numbers\n","rndom = [random.randint(1, len(test_set)) for x in range(10)]\n"," \n","# list of 10 sentencess on which to test the model\n","test_run = [test_set[i] for i in rndom]\n"," \n","# list of tagged words\n","test_run_base = [tup for sent in test_run for tup in sent]\n"," \n","# list of untagged words\n","test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n","\n","# testing 10 sentences to check the accuracy\n","start = time.time()\n","tagged_seq = Viterbi(test_tagged_words)\n","end = time.time()\n","difference = end - start\n"," \n","print(\"Time taken in seconds:\", difference)\n"," \n","# accuracy should be good enough (> 90%) to be a satisfactory model\n","check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n"," \n","accuracy = len(check) / len(tagged_seq)\n","print('Viterbi Algorithm Accuracy:', accuracy * 100)"]},{"cell_type":"code","execution_count":13,"id":"La23ju-odgYN","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10688,"status":"ok","timestamp":1667903091052,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"La23ju-odgYN","outputId":"39b16fbb-d34a-4e40-b3ad-a8fb55be38a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken in seconds: 10.212006568908691\n","Viterbi Algorithm Accuracy: 93.9359717175685\n"]}],"source":["# test the Viterbi algorithm on the entire test dataset\n","# list of tagged words\n","test_run_base = [tup for sent in test_set for tup in sent]\n"," \n","# list of untagged words\n","test_tagged_words = [tup[0] for sent in test_set for tup in sent]\n","\n","# testing 10 sentences to check the accuracy\n","start = time.time()\n","tagged_seq = Viterbi(test_tagged_words)\n","end = time.time()\n","difference = end - start\n"," \n","print(\"Time taken in seconds:\", difference)\n"," \n","# accuracy should be good enough (> 90%) to be a satisfactory model\n","check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n"," \n","accuracy = len(check) / len(tagged_seq)\n","print('Viterbi Algorithm Accuracy:', accuracy * 100)"]},{"cell_type":"code","execution_count":14,"id":"XqLP5WSZ9ZrG","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1667903091053,"user":{"displayName":"Kishlay Kumar","userId":"11767334481866808041"},"user_tz":-330},"id":"XqLP5WSZ9ZrG"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1d7T8RKG1GvkDO0ppmhNyx2CPlInwfPvB","timestamp":1667633480464},{"file_id":"1OsuOlenFA09Yww3Lw7hzT_bAdl8ZBsTG","timestamp":1667488672028}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.8 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"71378fec4b9b0ae51f784db9960b5ea07fb9042b7f7d69b6f196c430cecf08e0"}}},"nbformat":4,"nbformat_minor":5}
